### 信息量
數據的壓縮水平與信息量有關，信息量越大，越難以壓縮。 <br>
單個比特位的信息量或者單個位置的信息量，可以用該字符在該位置出現的概率的負相關來表示。公式: $h(x) = \log_2(\frac{1}{p(x)})$ 其中$h(x)$表示該位置為符號$x$時的信息量。$p(x)$表示符號$x$在該位置出現的概率。 <br>
字符串的信息量為每位信息量的和。公式: $h(s)=\sum h(b_i)$ <br>

參考：<br>

+   [无损压缩经典算法](https://blog.csdn.net/cordova/article/details/52928432)
### 信息熵
數據可以壓縮程度與信息熵有關，信息熵與信息量的關係是，當字符串的結構不再確定，信息量無法準確計算時，只能採取信息熵的方式。信息熵是信息量的數學期望。 <br>
熵在信息論中代表隨機變量不確定度的度量，公式$H(X)=-\sum_{x\in X} p(x)\log p(x)$。其中$p(x)$代表隨機變量$X$取符號$x$的概率。<br>
信息熵有三個性質：<br>

+   發生率越高的事件其熵約定，因為知道該事件發生不會大幅度減少不確定性
+   信息熵不能為負數，因為知道事件發生的信息不應該增加不確定性
+   累加性，多隨機事件同時發生的總不確定性的度量是各事件不確定性的度量的和。當然如果一個隨機變量包含另一個隨機變量的信息，那麼則存在互信息函數的概念。

參考：<br>

+   [信息熵是什么？](https://www.zhihu.com/question/22178202)
+   [阮一峰--推導有錯誤](http://www.ruanyifeng.com/blog/2014/09/information-entropy.html)
